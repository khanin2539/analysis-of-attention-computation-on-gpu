{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "num_SM = 200\n",
    "max_shared_memory_per_SM_bytes = 1111\n",
    "# theoricial numbers\n",
    "max_threads_per_SM = 2222\n",
    "\n",
    "iter = []\n",
    "smem_used_sQ = []\n",
    "smem_used_sK = []\n",
    "smem_used_sV = []\n",
    "used_shared_mem = []\n",
    "smem_used_sQ_layout = []\n",
    "smem_used_sK_layout = []\n",
    "smem_used_sV_layout = []\n",
    "num_sQ_per_iter = []\n",
    "num_sK_per_iter = []\n",
    "num_sV_per_iter = []\n",
    "tSrQ_per_iter = []\n",
    "tSrK_per_iter = []\n",
    "tOrVt_per_iter = []\n",
    "tSgS_per_iter = []\n",
    "acc_o_per_iter = []\n",
    "sVt_per_iter = []\n",
    "tSgS_per_iter = []\n",
    "gP_per_iter = []\n",
    "max_blocks_per_SM_shared_memory_ = []\n",
    "max_blocks_per_SM_threads_=[]\n",
    "threads_per_block = []\n",
    "blocks_per_SM = []\n",
    "iter_pair = []\n",
    "threads_per_SM = []\n",
    "percent_Smem_used_per_sm = []\n",
    "work_amount_per_thread = []\n",
    "total_threads = []\n",
    "\n",
    "for i in range(len(df['num_heads'])):\n",
    "    # Grid dimensions\n",
    "    grid_x = df['num_m_blocks'][i] \n",
    "    grid_y = df['batch_size'][i]\n",
    "    grid_z = df['num_heads'][i]  # Number of heads (params.h) 528 \n",
    "\n",
    "    # Total number of blocks in the grid\n",
    "    total_blocks_in_grid = grid_x * grid_y * grid_z\n",
    "\n",
    "    num_threads_used_per_block = 32*df['Nwarps'][i]\n",
    "    threads_per_block.append(num_threads_used_per_block)\n",
    "    sQ = df['kBlockM'][i]*df['headdim'][i]*2 \n",
    "    sKV = df['kBlockN'][i]*df['headdim'][i]*2 \n",
    "    kSmemSize = sQ + (2*sKV) \n",
    "    print(max_shared_memory_per_SM_bytes, kSmemSize)\n",
    "    max_blocks_per_SM_shared_memory = math.floor(237568 / kSmemSize) # in Bytes\n",
    "    max_blocks_per_SM_shared_memory_.append(max_blocks_per_SM_shared_memory)\n",
    "    max_blocks_per_SM_threads = math.floor(2048/num_threads_used_per_block)\n",
    "    max_blocks_per_SM_threads_.append(max_blocks_per_SM_threads)\n",
    "    # Determine the limiting factor\n",
    "    max_blocks_per_SM = min(max_blocks_per_SM_shared_memory, max_blocks_per_SM_threads)\n",
    "    blocks_per_SM.append(max_blocks_per_SM)\n",
    "    threads_per_SM_ = max_blocks_per_SM*num_threads_used_per_block\n",
    "    threads_per_SM.append(threads_per_SM_)\n",
    "   \n",
    "    # Total number of concurrent blocks\n",
    "    total_concurrent_blocks = max_blocks_per_SM * num_SM\n",
    "\n",
    "\n",
    "    # Calculate number of waves needed on each GPU\n",
    "    number_of_iterations = math.ceil(total_blocks_in_grid / total_concurrent_blocks)\n",
    "    work_per_thread = total_blocks_in_grid/threads_per_SM_\n",
    "    work_amount_per_thread.append(work_per_thread)\n",
    "    iter_pair.append([total_blocks_in_grid, total_concurrent_blocks, max_blocks_per_SM, max_blocks_per_SM_shared_memory, max_blocks_per_SM_threads])\n",
    "\n",
    "    iter.append(number_of_iterations)\n",
    "     # Q\n",
    "    actual_list_q = eval(df['sQ'][i])\n",
    "    # sQ_size = actual_list_q[0]*actual_list_q[1]\n",
    "    smem_used_sQ.append(sQ)\n",
    "    smem_used_sQ_layout.append(int(str(actual_list_q[0])+str(actual_list_q[1])))\n",
    "    actual_list_tSrQ = eval(df['tSrQ'][i])\n",
    "    tSrQ_per_iter.append(int(str(actual_list_tSrQ[0])+str(actual_list_tSrQ[1])))\n",
    "    mQ_size = df['headdim'][i]*df['num_heads'][i]*df['seqlen'][i]\n",
    "    num_sQ_per_iter.append((mQ_size/sQ))\n",
    "    # K\n",
    "    actual_list_k = eval(df['sK'][i])\n",
    "    # sK_size = actual_list_k[0]*actual_list_k[1]\n",
    "    smem_used_sK.append(sKV)\n",
    "    smem_used_sK_layout.append(int(str(actual_list_k[0])+str(actual_list_k[1])))\n",
    "    actual_list_tSrK = eval(df['tSrK'][i])\n",
    "    tSrK_per_iter.append(int(str(actual_list_tSrK[0])+str(actual_list_tSrK[1])))\n",
    "    mK_size = df['headdim'][i]*df['num_heads'][i]*df['seqlen'][i]\n",
    "    num_sK_per_iter.append((mK_size/sKV))\n",
    "    percent_Smem_used_per_sm.append(kSmemSize/237568)\n",
    "    # V\n",
    "    actual_list_v = eval(df['sV'][i])\n",
    "    # sV_size = actual_list_v[0]*actual_list_v[1]\n",
    "    smem_used_sV.append(sKV)\n",
    "    smem_used_sV_layout.append(int(str(actual_list_v[0])+str(actual_list_v[1])))\n",
    "    mV_size = df['headdim'][i]*df['num_heads'][i]*df['seqlen'][i]\n",
    "    actual_list_tOrVt = eval(df['tOrVt'][i])\n",
    "    tOrVt_per_iter.append(int(str(actual_list_tOrVt[0])+str(actual_list_tOrVt[1])))\n",
    "    actual_list_acc_o = eval(df['acc_o'][i])\n",
    "    acc_o_per_iter.append(int(str(actual_list_acc_o[0])+str(actual_list_acc_o[1])))\n",
    "    actual_list_sVt = eval(df['sVt'][i])\n",
    "    sVt_per_iter.append(int(str(actual_list_sVt[0])+str(actual_list_sVt[1])))\n",
    "    actual_list_tSgS = eval(df['tSgS'][i])\n",
    "    tSgS_per_iter.append(int(str(actual_list_tSgS[0])+str(actual_list_tSgS[1])))\n",
    "    actual_list_gP = eval(df['gP'][i])\n",
    "    gP_per_iter.append(int(str(actual_list_gP[0])+str(actual_list_gP[1])))\n",
    "    num_sV_per_iter.append((mV_size/sKV))\n",
    "    used_shared_mem.append(kSmemSize)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iter_wave_per_kernel_call'] = iter\n",
    "df['shared_mem_used_by_SQ_SK_SV'] = used_shared_mem\n",
    "df['sQ'] = smem_used_sQ_layout\n",
    "df['sK'] = smem_used_sK_layout\n",
    "df['sV'] = smem_used_sV_layout\n",
    "df['num_blocks_sQ_per_iter'] = num_sQ_per_iter\n",
    "df['num_blocks_sK_per_iter'] = num_sV_per_iter\n",
    "df['num_blocks_sV_per_iter'] = num_sK_per_iter\n",
    "df['tSrQ_layout'] = tSrQ_per_iter\n",
    "df['tOrVt_layout'] = tOrVt_per_iter\n",
    "df['tSrK_layout'] = tSrK_per_iter\n",
    "df['acc_o_layout'] = acc_o_per_iter\n",
    "df['sVt_layout'] = sVt_per_iter\n",
    "df['tSgS_layout'] = tSgS_per_iter\n",
    "df['gP_layout'] = gP_per_iter\n",
    "df['threads_per_block'] = threads_per_block\n",
    "df['max_blocks_per_SM_shared_memory'] = max_blocks_per_SM_shared_memory_\n",
    "df['max_blocks_per_SM_threads_'] = max_blocks_per_SM_threads_\n",
    "df['blocks_per_SM'] = blocks_per_SM\n",
    "df['threads_per_SM'] = threads_per_SM\n",
    "df['percent_Smem_used_per_sm'] = percent_Smem_used_per_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([ 'acc_o', 'tSrQ', 'tOrVt', 'tSrK', 'sVt', 'tSgS', 'gP', 'gV', 'gK', 'gQ'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete pytorch ones\n",
    "df = df[df['method'] != 'Pytorch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No casual mask and dropouts\n",
    "df_1 = df\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Separate the target from the features\n",
    "X = df_1.drop(columns=['fwd_TFLOPs/s'])\n",
    "y = df_1['fwd_TFLOPs/s']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.000001, random_state=42)\n",
    "\n",
    "df['Nwarps'].unique()\n",
    "X_test['Nwarps'].unique()\n",
    "# Initialize the RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance in RandomForest Regressor')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Display the feature importance dataframe\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Feature Importance\", dataframe=feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "# Partial Dependence Plots\n",
    "most_important_features = feature_importance_df['Feature'].head(20).tolist()\n",
    "\n",
    "\n",
    "# Plotting partial dependence for the top 2 features\n",
    "fig, ax = plt.subplots(figsize=(40, 24))\n",
    "PartialDependenceDisplay.from_estimator(model, X_train, features=most_important_features, ax=ax)\n",
    "plt.suptitle('Partial Dependence Plots for Top Features')\n",
    "plt.subplots_adjust(top=0.9)  # Adjust layout to make space for the title\n",
    "plt.show()\n",
    "\n",
    "#  Plotting partial dependence for the top 2 features in a grid\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "\n",
    "print(PartialDependenceDisplay.from_estimator(model, X_train, features=[['kBlockM', 'percent_Smem_used_per_sm']], kind='average', ax=ax))\n",
    "# Adjust layout to make space for everything\n",
    "plt.suptitle('Partial Dependence Plots for Top 2 Features', fontsize=16)\n",
    "# plt.subplots_adjust(left=0.1, bottom=1, right=0.9, top=0.85, wspace=0.2, hspace=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "print(PartialDependenceDisplay.from_estimator(model, X_train, features=[['max_blocks_per_SM_threads', 'Nwarps']], kind='average', ax=ax))\n",
    "# Adjust layout to make space for everything\n",
    "plt.suptitle('Partial Dependence Plots for Top 2 Features', fontsize=16)\n",
    "plt.subplots_adjust(left=0.1, bottom=1, right=0.9, top=0.85, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "print(PartialDependenceDisplay.from_estimator(model, X_train, features=[['threads_per_block', 'Nwarps']], kind='average', ax=ax))\n",
    "# Adjust layout to make space for everything\n",
    "plt.suptitle('Partial Dependence Plots for Top 2 Features', fontsize=16)\n",
    "plt.subplots_adjust(left=0.1, bottom=1, right=0.9, top=0.85, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "print(PartialDependenceDisplay.from_estimator(model, X_train, features=[['blocks_per_SM', 'Nwarps']], kind='average', ax=ax))\n",
    "# Adjust layout to make space for everything\n",
    "plt.suptitle('Partial Dependence Plots for Top 2 Features', fontsize=16)\n",
    "plt.subplots_adjust(left=0.1, bottom=1, right=0.9, top=0.85, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
